### 1. FT Instability from BERT to T5 & GPT
The high generalization variance leads to fine-tuning Instability in BERT, from the read paper in file, [ON THE STABILITY OF FINE-TUNING BERT: MISCONCEPTIONS, EXPLANATIONS, AND STRONG BASELINES](https://github.com/YCChu1995/Paper-Summary/blob/main/2006_On%20the%20Stability%20of%20Fine-tuning%20BERT%20-%20Misconceptions%2C%20Explanations%2C%20and%20Strong%20Baselines.md)<br>
The high generalization variance is also found in T5 and GPT.
- [2301.09820](https://arxiv.org/abs/2301.09820) A Stability Analysis of Fine-Tuning a Pre-Trained Model 
- [2302.07778](https://arxiv.org/abs/2302.07778) Measuring the Instability of Fine-Tuning

### 2. LoRA should tuning Q + V over Q + K + V 
https://arxiv.org/abs/2410.02247
