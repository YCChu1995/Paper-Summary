# Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet
> [2405](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)<br>
<div align=center><img src="/figures/2405.anthropic.01.png" style="height: 150px; width: auto;"/></div>

## Summary 
1. 

## Tech Insights 
1. Pre-encoder bias is useful on synthetic data from small `toy model`, but `removing pre-encoder bias` is beneficial for `real transformer` activations. ([Source](https://transformer-circuits.pub/2024/feb-update/index.html?utm_source=chatgpt.com#dict-learning-loss))

---

## Motivation 
Early work applied sparse autoencoders to `tiny models`, but it was unclear whether those techniques would `scale to real production models`.

## Chain of Thoughts

## Experiment
### 1.
### 2. 
- x
- y<br>
&rarr; y1 + y2 = y3
- z
